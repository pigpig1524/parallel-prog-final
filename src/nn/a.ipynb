{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaff2bf0",
   "metadata": {},
   "source": [
    "### Phần 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178f2ca",
   "metadata": {},
   "source": [
    "### Phần 2:\n",
    "##### Chuyển dữ liệu host ↔ device:\n",
    "Host đưa input cho device. Device xử lí trong nội bộ và trả về output cho host. Vậy ta sẽ có các hàm cudaMemcpy để thực hiện các tác vụ:  \n",
    "+ chuyển input từ host sang device  \n",
    "+ Khởi tạo các trọng số cho mô hình sẽ được thực hiện trên host rồi chuyển sang device.\n",
    "+ Chuyển output từ device sang host\n",
    "+ Chuyển giá trị hàm loss từ device sang host để giám sát quá trình huấn luyện mô hình.\n",
    "\n",
    "##### Phần tổ chức dữ liệu cho kernel:\n",
    "Như ta đã biết thì dữ liệu lưu theo index của `threadIdx`, `blockDim`, `blockIdx` chỉ có 3 chiều (x, y, z), trong khi đó, dữ liệu mà ta đang xử lí có tới 4 chiều (H, W, Channel, Batch). Ý tưởng của nhóm em là gộp chiều Channel với Batch lại và lưu và chiều z. Khi đó, một dữ liệu input có chiều (x,y,c,b):\n",
    "$$\\begin{align}\n",
    "\\text{flat\\_index} &= (b \\cdot C + c) \\cdot H \\cdot W + W \\cdot h + w \\\\\n",
    "&= ((b \\cdot C + c) \\cdot H + h) \\cdot W + w\n",
    "\\end{align}$$\n",
    "\n",
    "##### Tại sao trên các kernel ReLU lại chỉ sử dụng 1 chiều x trong khi các kernel khác thì sử dụng tới 3 chiều ?\n",
    "\n",
    "##### Tại sao index của chiều batch-channel lại được biểu diễn qua blockIdx.z mà không phải là blockIdx.z * blockDim.z + threadIdx.z ?\n",
    "\n",
    "\n",
    "##### Tại sao ko cần cudaDeviceSynchronize() sau mỗi kernel call trong forwardPass ?\n",
    "Vì tất cả job đều nằm trên cùng 1 stream 0 nên các hàm được nằm ở trước chắc chắn sẽ được thực hiện trước.\n",
    "\n",
    "##### Hiện tại backward pass chưa hợp lí, vì padding scheme của m khác với code gemini\n",
    "\n",
    "\n",
    "Trong hàm backward, các grad đã được chia trung bình cho toàn bộ pixel trong cả batch. Vậy mà trong hàm updateWeights, grad còn được chia cho batchSize 1 lần nữa. Liệu nó có bị dư ?\n",
    "\n",
    "\n",
    "##### Trong quá trình làm, liên tục bị gradient explosion nên đã áp dụng gradient clipping cho backprop ổn định hơn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f278b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
